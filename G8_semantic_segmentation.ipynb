{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 도로 영역을 찾자! - 세그멘테이션 모델 만들기\n",
    "\n",
    "## 평가 루브릭\n",
    "\n",
    "아래의 기준을 바탕으로 프로젝트를 평가합니다.\n",
    "\n",
    "평가문항\t상세기준\n",
    "1. U-Net을 통한 세그멘테이션 작업이 정상적으로 진행되었는가? : KITTI 데이터셋 구성, U-Net 모델 훈련, 결과물 시각화의 한사이클이 정상수행되어 세그멘테이션 결과이미지를 제출하였다.\n",
    "2. U-Net++ 모델이 성공적으로 구현되었는가? : U-Net++ 모델을 스스로 구현하여 학습 진행 후 세그멘테이션 결과까지 정상진행되었다.\n",
    "3. U-Net과 U-Net++ 두 모델의 성능이 정량적/정성적으로 잘 비교되었는가? : U-Net++ 의 세그멘테이션 결과 사진와 IoU 계산치를 U-Net과 비교하여 우월함을 확인하였다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 들어가며\n",
    "\n",
    "- 이번 시간에는 시맨틱 세그멘테이션(semantic segmentation)을 이용해서 자율주행차량이 주행해야 할 도로 영역을 찾는 상황을 가정하고 모델을 만들어보는 시간을 갖겠습니다. 앞서 강의 노드에서는 시맨틱 세그멘테이션을 위한 다양한 모델을 배워보았습니다. 이번 시간에는 U-Net을 사용해서 이미지가 입력되면 도로의 영역을 세그멘테이션 하는 모델을 만들어 볼 것입니다.\n",
    "\n",
    "- 최종적으로 만들어 볼 모델로 아래와 같은 결과를 만들어주세요! 아래 이미지에서는 입력 이미지 위에 도로 영역으로 인식한 영역을 흰색으로 오버레이 했습니다.\n",
    "\n",
    "- 실습목표\n",
    "    - 시맨틱 세그멘테이션 데이터셋을 전처리할 수 있습니다.\n",
    "    - 시맨틱 세그멘테이션 모델을 만들고 학습할 수 있습니다.\n",
    "    - 시맨틱 세그멘테이션 모델의 결과를 시각화할 수 있습니다.\n",
    "- 학습내용\n",
    "    - 시맨틱 세그멘테이션 데이터셋\n",
    "    - 시맨틱 세그멘테이션 모델\n",
    "    - 시맨틱 세그멘테이션 모델 시각화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 시맨틱 세그멘테이션 데이터셋\n",
    "## 이미지, 데이터 가져오기\n",
    "- 먼저 시맨틱 세그멘테이션(semantic segmentation)으로 도로의 영역을 분리하기 위해서는 도로의 영역을 라벨로 가진 데이터셋을 가지고 학습할 수 있도록 파싱해야 합니다. 아래 링크를 통해서 물체 검출(object detection)으로 사용했던 KITTI 데이터셋의 세그멘테이션 데이터를 다운로드 합니다.\n",
    "\n",
    "- \\$ mkdir -p ~/aiffel/semantic_segmentation/data\n",
    "- \\$ wget https://s3.eu-central-1.amazonaws.com/avg-kitti/data_semantics.zip\n",
    "- \\$ unzip data_semantics.zip -d ~/aiffel/semantic_segmentation/data\n",
    "\n",
    "- 아래는 데이터셋에서 확인할 수있는 이미지와 라벨입니다. 지금까지 보던 라벨 데이터와 다르게 세그멘테이션 데이터는 이미지 형태의 라벨으로 되어있습니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 로더(data loader) 만들기\n",
    "- 이제 이미지와 데이터셋이 주어졌으니 모델을 학습시킬 수 있는 데이터 로더(data loader)를 만들어봅시다. 로더는 입력값 (224, 224), 출력값 (224, 244) 크기를 갖는 모델을 학습시킬 수 있도록 데이터셋을 파싱해야 합니다.\n",
    "\n",
    "- 이때 데이터 로더에 augmentation을 적용해 봅시다. 오늘은 파이썬에서 많이 사용되는 albumentations을 사용해 보겠습니다. 하지만 Augmentation의 경우 imgaug 등 다른 라이브러리를 활용하셔도 됩니다.\n",
    "\n",
    "- 시맨틱 세그멘테이션의 이미지 라벨의 각 pixel 값은 의미를 갖습니다. 그렇다면 오늘 우리가 시맨틱 세그멘테이션으로 찾아내야 할 도로의 label은  어떤 값일까요? 7입니다. \n",
    "\n",
    "- \\$ pip install albumentations\n",
    "\n",
    "- 또한 학습셋의 일정량을 검증 데이터셋(validation dataset)으로 활용할 수 있도록 해야 합니다.\n",
    "\n",
    "- 만든 후에는 직접 데이터셋이 잘 파싱되어 나오는지 꼭 확인해보세요. 데이터셋에 오류가 없어야 성능이 안 나오더라도 문제를 찾아내기 쉽습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#필요한 라이브러리를 로드합니다. \n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from glob import glob\n",
    "\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Albumentation 의 사용법을 간단히 알아보겠습니다. 아래는 Albumentation에서 다양한 augmentation 기법을 확률적으로 적용할 수 있게 해주는 Compose()의 활용예입니다. imgaug의 Somtimes()와 유사한 기능입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from albumentations import  HorizontalFlip, RandomSizedCrop, Compose, OneOf, Resize\n",
    "\n",
    "def build_augmentation(is_train=True):\n",
    "    if is_train:    # 훈련용 데이터일 경우\n",
    "        return Compose([\n",
    "                        HorizontalFlip(p=0.5),    # 50%의 확률로 좌우대칭\n",
    "                        RandomSizedCrop(         # 50%의 확률로 RandomSizedCrop\n",
    "                            min_max_height=(300, 370),\n",
    "                            w2h_ratio=370/1242,\n",
    "                            height=224,\n",
    "                            width=224,\n",
    "                            p=0.5\n",
    "                            ),\n",
    "                        Resize(              # 입력이미지를 224X224로 resize\n",
    "                            width=224,\n",
    "                            height=224\n",
    "                            )\n",
    "                        ])\n",
    "      return Compose([      # 테스트용 데이터일 경우에는 224X224로 resize만 수행합니다. \n",
    "                    Resize(\n",
    "                        width=224,\n",
    "                        height=224\n",
    "                        )\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "dir_path = os.getenv('HOME')+'/aiffel/semantic_segmentation/data/training'\n",
    "\n",
    "augmentation = build_augmentation()\n",
    "input_images = glob(os.path.join(dir_path, \"image_2\", \"*.png\"))\n",
    "\n",
    "# 훈련 데이터셋에서 5개만 가져와 augmentation을 적용해 봅시다.  \n",
    "plt.figure(figsize=(12, 20))\n",
    "for i in range(5):\n",
    "    image = imread(input_images[i]) \n",
    "    image_data = {\"image\":image}\n",
    "    resized = augmentation(**image_data, is_train=False)\n",
    "    processed = augmentation(**image_data)\n",
    "    plt.subplot(5, 2, 2*i+1)\n",
    "    plt.imshow(resized[\"image\"])  # 왼쪽이 원본이미지\n",
    "    plt.subplot(5, 2, 2*i+2)\n",
    "    plt.imshow(processed[\"image\"])  # 오른쪽이 augment된 이미지\n",
    "  \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 데이터셋을 구성해 봅시다. 이번에는 tf.keras.utils.Sequence를 상속받은 generator 형태로 데이터를 구성해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KittiGenerator(tf.keras.utils.Sequence):\n",
    "  '''\n",
    "  KittiGenerator는 tf.keras.utils.Sequence를 상속받습니다.\n",
    "  우리가 KittiDataset을 원하는 방식으로 preprocess하기 위해서 Sequnce를 커스텀해 사용합니다.\n",
    "  '''\n",
    "  def __init__(self, \n",
    "               dir_path,\n",
    "               batch_size=16,\n",
    "               img_size=(224, 224, 3),\n",
    "               output_size=(224, 224),\n",
    "               is_train=True,\n",
    "               augmentation=None):\n",
    "    '''\n",
    "    dir_path: dataset의 directory path입니다.\n",
    "    batch_size: batch_size입니다.\n",
    "    img_size: preprocess에 사용할 입력이미지의 크기입니다.\n",
    "    output_size: ground_truth를 만들어주기 위한 크기입니다.\n",
    "    is_train: 이 Generator가 학습용인지 테스트용인지 구분합니다.\n",
    "    augmentation: 적용하길 원하는 augmentation 함수를 인자로 받습니다.\n",
    "    '''\n",
    "    self.dir_path = dir_path\n",
    "    self.batch_size = batch_size\n",
    "    self.is_train = is_train\n",
    "    self.dir_path = dir_path\n",
    "    self.augmentation = augmentation\n",
    "    self.img_size = img_size\n",
    "    self.output_size = output_size\n",
    "\n",
    "    # load_dataset()을 통해서 kitti dataset의 directory path에서 라벨과 이미지를 확인합니다.\n",
    "    self.data = self.load_dataset()\n",
    "\n",
    "    def load_dataset(self):\n",
    "        # kitti dataset에서 필요한 정보(이미지 경로 및 라벨)를 directory에서 확인하고 로드하는 함수입니다.\n",
    "        # 이때 is_train에 따라 test set을 분리해서 load하도록 해야합니다.\n",
    "        input_images = glob(os.path.join(self.dir_path, \"image_2\", \"*.png\"))\n",
    "        label_images = glob(os.path.join(self.dir_path, \"semantic\", \"*.png\"))\n",
    "        input_images.sort()\n",
    "        label_images.sort()\n",
    "        assert len(input_images) == len(label_images)\n",
    "        data = [ _ for _ in zip(input_images, label_images)]\n",
    "\n",
    "        if self.is_train:\n",
    "            return data[:-30]\n",
    "        return data[-30:]\n",
    "    \n",
    "    def __len__(self):\n",
    "        # Generator의 length로서 전체 dataset을 batch_size로 나누고 소숫점 첫째자리에서 올림한 값을 반환합니다.\n",
    "        return math.ceil(len(self.data) / self.batch_size)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # 입력과 출력을 만듭니다.\n",
    "        # 입력은 resize및 augmentation이 적용된 input image이고 \n",
    "        # 출력은 semantic label입니다.\n",
    "        batch_data = self.data[\n",
    "                               index*self.batch_size:\n",
    "                               (index + 1)*self.batch_size\n",
    "                               ]\n",
    "        inputs = np.zeros([self.batch_size, *self.img_size])\n",
    "        outputs = np.zeros([self.batch_size, *self.output_size])\n",
    "\n",
    "        for i, data in enumerate(batch_data):\n",
    "            input_img_path, output_path = data\n",
    "            _input = imread(input_img_path)\n",
    "            _output = imread(output_path)\n",
    "            _output = (_output==7).astype(np.uint8)*1\n",
    "            data = {\n",
    "                \"image\": _input,\n",
    "                \"mask\": _output,\n",
    "            }\n",
    "            augmented = self.augmentation(**data)\n",
    "            inputs[i] = augmented[\"image\"]/255\n",
    "            outputs[i] = augmented[\"mask\"]\n",
    "        return inputs, outputs\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        # 한 epoch가 끝나면 실행되는 함수입니다. 학습중인 경우에 순서를 random shuffle하도록 적용한 것을 볼 수 있습니다.\n",
    "        self.indexes = np.arange(len(self.data))\n",
    "        if self.is_train == True :\n",
    "            np.random.shuffle(self.indexes)\n",
    "            return self.indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentation = build_augmentation()\n",
    "test_preproc = build_augmentation(is_train=False)\n",
    "        \n",
    "train_generator = KittiGenerator(\n",
    "    dir_path, \n",
    "    augmentation=augmentation,\n",
    ")\n",
    "\n",
    "test_generator = KittiGenerator(\n",
    "    dir_path, \n",
    "    augmentation=test_preproc,\n",
    "    is_train=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 시맨틱 세그멘테이션 모델\n",
    "\n",
    "![title](u-net_1kfpgqE.max-800x600.png) \n",
    "\n",
    "# 모델 구조 만들기\n",
    "- 시맨틱 세그멘테이션을 위한 모델을 만들어주세요. 세그멘테이션 모델 중 구조상 비교적 구현이 단순한 U-Net을 구현해 봅시다.\n",
    "\n",
    "- 이때 입력 이미지의 크기는 위에서 만든 데이터셋에 맞춰서 만들어주세요.\n",
    "\n",
    "- 사용에 필요한 레이어와 연산은 다음과 같습니다. 그리고 필요에 따라서 Dropout등의 다른 레이어를 적용해보세요.\n",
    "\n",
    "    - Conv2D, UpSampling2D, MaxPooling2D, concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_shape=(224, 224, 3)):\n",
    "    inputs = Input(input_shape)\n",
    "    conv1 = Conv2D(64, 3, activation='relu', padding='same',kernel_initializer='he_normal')(inputs)\n",
    "    conv1 = Conv2D(64, 3, activation='relu', padding='same',kernel_initializer='he_normal')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    conv2 = Conv2D(128, 3, activation='relu', padding='same',kernel_initializer='he_normal')(pool1)\n",
    "    conv2 = Conv2D(128, 3, activation='relu', padding='same',kernel_initializer='he_normal')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    conv3 = Conv2D(256, 3, activation='relu', padding='same',kernel_initializer='he_normal')(pool2)\n",
    "    conv3 = Conv2D(256, 3, activation='relu', padding='same',kernel_initializer='he_normal')(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    conv4 = Conv2D(512, 3, activation='relu', padding='same',kernel_initializer='he_normal')(pool3)\n",
    "    conv4 = Conv2D(512, 3, activation='relu', padding='same',kernel_initializer='he_normal')(conv4)\n",
    "    drop4 = Dropout(0.5)(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
    "\n",
    "    conv5 = Conv2D(1024, 3, activation='relu', padding='same',kernel_initializer='he_normal')(pool4)  \n",
    "    conv5 = Conv2D(1024, 3, activation='relu', padding='same',kernel_initializer='he_normal')(conv5)\n",
    "\n",
    "    drop5 = Dropout(0.5)(conv5)\n",
    "    up6 = Conv2D(512, 2, activation='relu', padding='same',kernel_initializer='he_normal')(UpSampling2D(size = (2,2))(drop5)) \n",
    "    merge6 = concatenate([drop4,up6], axis = 3)\n",
    "    conv6 = Conv2D(512, 3, activation='relu', padding='same',kernel_initializer='he_normal')(merge6)\n",
    "    conv6 = Conv2D(512, 3, activation='relu', padding='same',kernel_initializer='he_normal')(conv6)\n",
    "\n",
    "    up7 = Conv2D(256, 2, activation='relu', padding='same',kernel_initializer='he_normal')(UpSampling2D(size = (2,2))(conv6))\n",
    "    merge7 = concatenate([conv3,up7], axis = 3)\n",
    "    conv7 = Conv2D(256, 3, activation='relu', padding='same',kernel_initializer='he_normal')(merge7)\n",
    "    conv7 = Conv2D(256, 3, activation='relu', padding='same',kernel_initializer='he_normal')(conv7)\n",
    "\n",
    "    up8 = Conv2D(128, 2, activation='relu', padding='same',kernel_initializer='he_normal')(UpSampling2D(size = (2,2))(conv7))\n",
    "    merge8 = concatenate([conv2,up8], axis = 3)\n",
    "    conv8 = Conv2D(128, 3, activation='relu', padding='same',kernel_initializer='he_normal')(merge8)\n",
    "    conv8 = Conv2D(128, 3, activation='relu', padding='same',kernel_initializer='he_normal')(conv8)\n",
    "    up9 = Conv2D(64, 2, activation='relu', padding='same',kernel_initializer='he_normal')(UpSampling2D(size = (2,2))(conv8))\n",
    "    merge9 = concatenate([conv1,up9], axis = 3)\n",
    "    conv9 = Conv2D(64, 3, activation='relu', padding='same',kernel_initializer='he_normal')(merge9)\n",
    "    conv9 = Conv2D(64, 3, activation='relu', padding='same',kernel_initializer='he_normal')(conv9)  \n",
    "    conv9 = Conv2D(2, 3, activation='relu', padding='same',kernel_initializer='he_normal')(conv9)     \n",
    "    conv10 = Conv2D(1, 1, activation='sigmoid')(conv9)\n",
    "\n",
    "    model = Model(inputs = inputs, outputs = conv10)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 학습하기\n",
    "- 이제 모델을 만들었으니 위에서 만들어본 데이터셋과 학습을 해봅시다! 적절한 learning rate와 epoch를 찾아서 모델을 학습하고 저장해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model()\n",
    "model.compile(optimizer = Adam(lr = 1e-4), loss = 'binary_crossentropy')\n",
    "model.fit_generator(\n",
    "     generator=train_generator,\n",
    "     validation_data=test_generator,\n",
    "     steps_per_epoch=len(train_generator),\n",
    "     epochs=100,\n",
    " )\n",
    "\n",
    "model_path = dir_path + '/seg_model_unet.h5'\n",
    "model.save(model_path)  #학습한 모델을 저장해 주세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 시맨틱 세그멘테이션 모델 시각화\n",
    "- 이번에는 학습한 모델의 결과를 눈으로 확인해볼 차례입니다. 테스트 셋은 이미지를 제공하지만 정답 데이터를 제공하지 않으니 눈으로 확인할 수 있도록 모델이 추론(inference)한 결과를 우리의 눈으로 볼 수 있는 세그멘테이션 이미지로 만들어주세요!\n",
    "\n",
    "- 이때 입력 이미지와 라벨을 한번에 볼 수 있도록 모델의 출력값을 입력 이미지 위에 겹쳐서 보이기, 즉 오버레이(overray) 해 주세요. PIL 패키지를 사용하신다면 Image.blend를 활용하실 수 있습니다.\n",
    "\n",
    "- (참고) 이전 스텝에서 저장된 모델은 이후 아래와 같이 로드해서 활용할 수 있습니다.\n",
    "- model = tf.keras.models.load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output(model, preproc, image_path, output_path):\n",
    "    origin_img = imread(image_path)\n",
    "    data = {\"image\":origin_img}\n",
    "    processed = preproc(**data)\n",
    "    output = model(np.expand_dims(processed[\"image\"]/255,axis=0))\n",
    "    output = (output[0].numpy()>0.5).astype(np.uint8).squeeze(-1)*255  #0.5라는 threshold를 변경하면 도로인식 결과범위가 달라집니다.\n",
    "    output = Image.fromarray(output)\n",
    "    background = Image.fromarray(origin_img).convert('RGBA')\n",
    "    output = output.resize((origin_img.shape[1], origin_img.shape[0])).convert('RGBA')\n",
    "    output = Image.blend(background, output, alpha=0.5)\n",
    "    output.show()\n",
    "    return output\n",
    " \n",
    "\n",
    "# 완성한 뒤에는 시각화한 결과를 눈으로 확인해봅시다!\n",
    "i = 1    # i값을 바꾸면 테스트용 파일이 달라집니다. \n",
    "get_output(\n",
    "     model, \n",
    "     test_preproc,\n",
    "     image_path=dir_path + f'/image_2/00{str(i).zfill(4)}_10.png',\n",
    "     output_path=dir_path + f'./result_{str(i).zfill(3)}.png'\n",
    " )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 세그멘테이션이 성능을 정량적으로 측정하기 위한 방법으로 IoU(Intersection over Union)를 계산하게 됩니다. IoU를 계산하는 방법은 지난 시간에 소개한 바 있으므로 해당 내용을 활용하여 구현해 보겠습니다.\n",
    "\n",
    "- IoU를 계산하려면 모델이 도로 영역이라고 판단한 부분이 1로, 나머지 부분이 0으로 표시된 행렬, 그리고 라벨 데이터에서 도로 영역이 1, 나머지 부분이 0으로 표시된 행렬이 필요합니다. 각각을 prediction, target이라고 불렀을 때 이를 계산하는 함수를 구현해 보겠습니다. 위에 구현했던 get_output을 좀더 확장해서 output, prediction, target을 함께 리턴하도록 구현해 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_iou_score(target, prediction):\n",
    "    intersection = np.logical_and(target, prediction)\n",
    "    union = np.logical_or(target, prediction)\n",
    "    iou_score = float(np.sum(intersection)) / float(np.sum(union))\n",
    "    print('IoU : %f' % iou_score )\n",
    "    return iou_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output(model, preproc, image_path, output_path, label_path):\n",
    "    origin_img = imread(image_path)\n",
    "    data = {\"image\":origin_img}\n",
    "    processed = preproc(**data)\n",
    "    output = model(np.expand_dims(processed[\"image\"]/255,axis=0))\n",
    "    output = (output[0].numpy()>=0.5).astype(np.uint8).squeeze(-1)*255  #0.5라는 threshold를 변경하면 도로인식 결과범위가 달라집니다.\n",
    "    prediction = output/255   # 도로로 판단한 영역\n",
    "    \n",
    "    output = Image.fromarray(output)\n",
    "    background = Image.fromarray(origin_img).convert('RGBA')\n",
    "    output = output.resize((origin_img.shape[1], origin_img.shape[0])).convert('RGBA')\n",
    "    output = Image.blend(background, output, alpha=0.5)\n",
    "    output.show()   # 도로로 판단한 영역을 시각화!\n",
    "     \n",
    "    if label_path:   \n",
    "        label_img = imread(label_path)\n",
    "        label_data = {\"image\":label_img}\n",
    "        label_processed = preproc(**label_data)\n",
    "        label_processed = label_processed[\"image\"]\n",
    "        target = (label_processed == 7).astype(np.uint8)*1   # 라벨에서 도로로 기재된 영역\n",
    "\n",
    "        return output, prediction, target\n",
    "    else:\n",
    "        return output, prediction, _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 완성한 뒤에는 시각화한 결과를 눈으로 확인해봅시다!\n",
    "i = 1    # i값을 바꾸면 테스트용 파일이 달라집니다. \n",
    "output, prediction, target = get_output(\n",
    "     model, \n",
    "     test_preproc,\n",
    "     image_path=dir_path + f'/image_2/00{str(i).zfill(4)}_10.png',\n",
    "     output_path=dir_path + f'./result_{str(i).zfill(3)}.png',\n",
    "     label_path=dir_path + f'/semantic/00{str(i).zfill(4)}_10.png'\n",
    " )\n",
    "\n",
    "calculate_iou_score(target, prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 프로젝트 : 개선된 U-Net 모델 만들기\n",
    "- U-Net을 통한 시맨틱 세그멘테이션 결과가 충분히 만족스러우신가요? 어느정도 동작하는 것 같긴 하지만 좀더 개선할 여지도 보일 것입니다.\n",
    "\n",
    "- 2018년에 U-Net++라는 논문이 나왔습니다. 이 논문은 기존에 아주 단순하면서도 세그멘테이션에서 효과를 발휘했던 U-Net의 네트워크 구조에 DenseNet의 아이디어를 가미하여 성능을 개선한 모델입니다.\n",
    "\n",
    "- 그래서 모델의 구조 자체는 아래 그림에서 보는 것처럼 별다른 설명이 없이도 직관적으로 이해가 가능한 수준입니다. 오늘 소개되었던 U-Net의 모델 코드를 조금만 수정 확장하면 충분히 구현할 수 있을 것입니다. 그래서 오늘의 과제는 바로 U-Net++ 모델을 스스로의 힘으로 직접 구현해 보고, U-Net을 활용했던 도로 세그멘테이션 태스크에 적용하여 U-Net을 썼을 때보다 성능이 향상되었음을 확인해 보는 것입니다. 정성적으로는 두 모델의 세그멘테이션 결과를 시각화해서 비교해 볼 수 있을 것이고, 정량적으로는 동일 이미지에 대한 IoU값을 비교해 보면 될 것입니다.\n",
    "\n",
    "![title](GC-5-P-UNPP.max-800x600.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1. KITTI 데이터셋 수집과 구축\n",
    "- 다운받아 둔 KITTI 데이터에 data augmentation을 적용한 형태로 데이터셋을 구축합니다. 이때 주의할 점이 있습니다. U-Net++는 내부적인 메모리 사용량이 U-Net보다 꽤 많아집니다. 8GB의 GPU 메모리를 가진 모델의 경우 학습데이터의 배치 사이즈를 16->4 로 줄여서 설정하시기를 권합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2. U-Net++ 모델의 구현\n",
    "- U-Net의 모델 구조와 소스코드를 면밀히 비교해 보다 보면, U-Net++를 어떻게 구현할 수 있을지에 대한 방안을 떠올릴 수 있을 것입니다. 이 과정을 통해 U-Net 자체에 대한 이해도도 증진될 것입니다.\n",
    "- 그 외 적절히 U-Net이 백본구조, 기타 파라미터 변경 등을 통해 추가적인 성능향상이 가능할수도 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3. U-Net 과 U-Net++ 모델이 수행한 세그멘테이션 결과 분석\n",
    "- 두 모델의 정량적, 정성적 성능을 비교해 봅시다. 시각화, IoU 계산 등을 체계적으로 시도해 보면 차이를 발견하실 수 있을 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
